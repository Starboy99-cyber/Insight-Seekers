{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "590a38f5-d685-420a-a1fc-c7ffdf8b17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9fe9914a-b917-40b7-b67d-a360e135a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train= pd.read_csv(r\"C:\\Users\\Aditi\\Downloads\\final_train_.csv\")\n",
    "d_test=pd.read_csv(r\"C:\\Users\\Aditi\\Downloads\\final_test_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b404ba4-2138-4d0c-852d-eba7cb4e3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train=d_train.drop(columns=[\"HH_ID\"],errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed0ced07-cd9e-4765-8fec-ac6d13aed952",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test=d_test.drop(columns=[\"HH_ID\"],errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b74ab5c9-99bf-4358-8153-15638568a0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>HH Size (For FDQ)</th>\n",
       "      <th>Male_Count</th>\n",
       "      <th>Female_Count</th>\n",
       "      <th>Other_Count</th>\n",
       "      <th>Age_0_18</th>\n",
       "      <th>Age_18_60</th>\n",
       "      <th>Age_60_above</th>\n",
       "      <th>Highest educational level attained_head</th>\n",
       "      <th>Total year of education completed_head</th>\n",
       "      <th>...</th>\n",
       "      <th>Is_HH_Have_Washing_machine_0</th>\n",
       "      <th>Is_HH_Have_Washing_machine_1</th>\n",
       "      <th>Is_HH_Have_Airconditioner_aircooler_0</th>\n",
       "      <th>Is_HH_Have_Airconditioner_aircooler_1</th>\n",
       "      <th>Marital Status_head_1</th>\n",
       "      <th>Marital Status_head_2</th>\n",
       "      <th>Marital Status_head_3</th>\n",
       "      <th>Marital Status_head_4</th>\n",
       "      <th>Whether used internet from any location during last 30 days_1</th>\n",
       "      <th>Whether used internet from any location during last 30 days_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 408 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  HH Size (For FDQ)  Male_Count  Female_Count  Other_Count  \\\n",
       "0           0                  2           1             1            0   \n",
       "1           1                  4           2             2            0   \n",
       "2           2                  3           1             2            0   \n",
       "3           3                  4           1             3            0   \n",
       "4           4                  3           1             2            0   \n",
       "\n",
       "   Age_0_18  Age_18_60  Age_60_above  Highest educational level attained_head  \\\n",
       "0         0          1             1                                        4   \n",
       "1         2          2             0                                        7   \n",
       "2         2          1             0                                        1   \n",
       "3         1          2             1                                       11   \n",
       "4         0          3             0                                        3   \n",
       "\n",
       "   Total year of education completed_head  ...  Is_HH_Have_Washing_machine_0  \\\n",
       "0                                       7  ...                             1   \n",
       "1                                      13  ...                             1   \n",
       "2                                       0  ...                             1   \n",
       "3                                      18  ...                             1   \n",
       "4                                       4  ...                             1   \n",
       "\n",
       "   Is_HH_Have_Washing_machine_1  Is_HH_Have_Airconditioner_aircooler_0  \\\n",
       "0                             0                                      1   \n",
       "1                             0                                      1   \n",
       "2                             0                                      1   \n",
       "3                             0                                      0   \n",
       "4                             0                                      0   \n",
       "\n",
       "   Is_HH_Have_Airconditioner_aircooler_1  Marital Status_head_1  \\\n",
       "0                                      0                      0   \n",
       "1                                      0                      0   \n",
       "2                                      0                      0   \n",
       "3                                      1                      0   \n",
       "4                                      1                      0   \n",
       "\n",
       "   Marital Status_head_2  Marital Status_head_3  Marital Status_head_4  \\\n",
       "0                      1                      0                      0   \n",
       "1                      1                      0                      0   \n",
       "2                      0                      1                      0   \n",
       "3                      1                      0                      0   \n",
       "4                      0                      1                      0   \n",
       "\n",
       "   Whether used internet from any location during last 30 days_1  \\\n",
       "0                                                  0               \n",
       "1                                                  1               \n",
       "2                                                  0               \n",
       "3                                                  1               \n",
       "4                                                  1               \n",
       "\n",
       "   Whether used internet from any location during last 30 days_2  \n",
       "0                                                  1              \n",
       "1                                                  0              \n",
       "2                                                  1              \n",
       "3                                                  0              \n",
       "4                                                  0              \n",
       "\n",
       "[5 rows x 408 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9e0b2ed-5532-4a46-ac85-652f56f5ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = d_train.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8e3ff3a-58fb-4cf5-9116-a78fe4a6c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = d_test.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6494245-2de5-4ac5-895b-288da5413493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HH Size (For FDQ)</th>\n",
       "      <th>Male_Count</th>\n",
       "      <th>Female_Count</th>\n",
       "      <th>Other_Count</th>\n",
       "      <th>Age_0_18</th>\n",
       "      <th>Age_18_60</th>\n",
       "      <th>Age_60_above</th>\n",
       "      <th>Highest educational level attained_head</th>\n",
       "      <th>Total year of education completed_head</th>\n",
       "      <th>Highest educational level attained_median</th>\n",
       "      <th>...</th>\n",
       "      <th>Is_HH_Have_Washing_machine_0</th>\n",
       "      <th>Is_HH_Have_Washing_machine_1</th>\n",
       "      <th>Is_HH_Have_Airconditioner_aircooler_0</th>\n",
       "      <th>Is_HH_Have_Airconditioner_aircooler_1</th>\n",
       "      <th>Marital Status_head_1</th>\n",
       "      <th>Marital Status_head_2</th>\n",
       "      <th>Marital Status_head_3</th>\n",
       "      <th>Marital Status_head_4</th>\n",
       "      <th>Whether used internet from any location during last 30 days_1</th>\n",
       "      <th>Whether used internet from any location during last 30 days_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>8.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 407 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HH Size (For FDQ)  Male_Count  Female_Count  Other_Count  Age_0_18  \\\n",
       "0                  2           1             1            0         0   \n",
       "1                  4           2             2            0         2   \n",
       "2                  3           1             2            0         2   \n",
       "3                  4           1             3            0         1   \n",
       "4                  3           1             2            0         0   \n",
       "\n",
       "   Age_18_60  Age_60_above  Highest educational level attained_head  \\\n",
       "0          1             1                                        4   \n",
       "1          2             0                                        7   \n",
       "2          1             0                                        1   \n",
       "3          2             1                                       11   \n",
       "4          3             0                                        3   \n",
       "\n",
       "   Total year of education completed_head  \\\n",
       "0                                       7   \n",
       "1                                      13   \n",
       "2                                       0   \n",
       "3                                      18   \n",
       "4                                       4   \n",
       "\n",
       "   Highest educational level attained_median  ...  \\\n",
       "0                                        4.0  ...   \n",
       "1                                        4.0  ...   \n",
       "2                                        4.0  ...   \n",
       "3                                        8.5  ...   \n",
       "4                                        7.0  ...   \n",
       "\n",
       "   Is_HH_Have_Washing_machine_0  Is_HH_Have_Washing_machine_1  \\\n",
       "0                             1                             0   \n",
       "1                             1                             0   \n",
       "2                             1                             0   \n",
       "3                             1                             0   \n",
       "4                             1                             0   \n",
       "\n",
       "   Is_HH_Have_Airconditioner_aircooler_0  \\\n",
       "0                                      1   \n",
       "1                                      1   \n",
       "2                                      1   \n",
       "3                                      0   \n",
       "4                                      0   \n",
       "\n",
       "   Is_HH_Have_Airconditioner_aircooler_1  Marital Status_head_1  \\\n",
       "0                                      0                      0   \n",
       "1                                      0                      0   \n",
       "2                                      0                      0   \n",
       "3                                      1                      0   \n",
       "4                                      1                      0   \n",
       "\n",
       "   Marital Status_head_2  Marital Status_head_3  Marital Status_head_4  \\\n",
       "0                      1                      0                      0   \n",
       "1                      1                      0                      0   \n",
       "2                      0                      1                      0   \n",
       "3                      1                      0                      0   \n",
       "4                      0                      1                      0   \n",
       "\n",
       "   Whether used internet from any location during last 30 days_1  \\\n",
       "0                                                  0               \n",
       "1                                                  1               \n",
       "2                                                  0               \n",
       "3                                                  1               \n",
       "4                                                  1               \n",
       "\n",
       "   Whether used internet from any location during last 30 days_2  \n",
       "0                                                  1              \n",
       "1                                                  0              \n",
       "2                                                  1              \n",
       "3                                                  0              \n",
       "4                                                  0              \n",
       "\n",
       "[5 rows x 407 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f075a88-74cc-4444-8622-8d4228d88d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92ac8b47-7125-4c8f-827e-91d5a4ba6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_log_test= pd.read_excel(r\"C:\\Users\\Aditi\\Downloads\\Target_test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2169098e-c78a-40e5-80b1-08bf79a5b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test=d_log_test[\"log_totalexpense\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fe8cbceb-c7f8-4b77-9aac-f7acb588c90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         8.377477\n",
       "1         9.443923\n",
       "2         9.380547\n",
       "3        10.135567\n",
       "4         9.553327\n",
       "           ...    \n",
       "52345     9.271024\n",
       "52346     9.601100\n",
       "52347    10.355267\n",
       "52348     9.280217\n",
       "52349     9.607088\n",
       "Name: log_totalexpense, Length: 52350, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d58b805-d256-4946-b998-eaf918eeb14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_log_train=pd.read_excel(r\"C:\\Users\\Aditi\\Downloads\\Target_train.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6a68573-69eb-4253-b41a-5cf285320b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train=d_log_train[\"log_TotalExpense\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4e4bbf1-99f8-444f-a16b-be289dacbc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TotalExpense</th>\n",
       "      <th>log_TotalExpense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4252.405088</td>\n",
       "      <td>8.355240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4900.391389</td>\n",
       "      <td>8.497070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34304.682970</td>\n",
       "      <td>10.443037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18091.958900</td>\n",
       "      <td>9.803223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23454.688850</td>\n",
       "      <td>10.062826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TotalExpense  log_TotalExpense\n",
       "0   4252.405088          8.355240\n",
       "1   4900.391389          8.497070\n",
       "2  34304.682970         10.443037\n",
       "3  18091.958900          9.803223\n",
       "4  23454.688850         10.062826"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_log_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "099b5c29-5667-43e6-89a5-7618cd42a37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          8.355240\n",
       "1          8.497070\n",
       "2         10.443037\n",
       "3          9.803223\n",
       "4         10.062826\n",
       "            ...    \n",
       "209391     8.933186\n",
       "209392     9.559184\n",
       "209393     9.781227\n",
       "209394     9.999128\n",
       "209395    10.091444\n",
       "Name: log_TotalExpense, Length: 209396, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "002e40c5-f8b0-4698-b3a3-0ee697b6ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X (features) and y (target)\n",
    "X_train = d_train # Use all features except TotalExpense\n",
    "y_train = log_train # Log-transformed target\n",
    "\n",
    "X_test = d_test  # Use all features except TotalExpense\n",
    "y_test = log_test  # Log-transformed target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "62639f34-314c-4c9a-aee1-27864466b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost on Log-Transformed Target → RMSE: 9205.1461, R² Score: 0.5448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_log = xgb_model.predict(X_test)\n",
    "\n",
    "# Convert log-transformed predictions back to original scale\n",
    "y_pred_original = np.expm1(y_pred_log)\n",
    "y_test_original = np.expm1(y_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "rmse = mean_squared_error(y_test_original, y_pred_original, squared=False)\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "print(f\"XGBoost on Log-Transformed Target → RMSE: {rmse:.4f}, R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54afd4cc-d8aa-402f-8236-aca544631ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Convert categorical columns to category dtype\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "X_train[categorical_cols] = X_train[categorical_cols].astype(\"category\")\n",
    "X_test[categorical_cols] = X_test[categorical_cols].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16c95811-1f5d-424f-ad1a-db8cb6009837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify object (string) columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Convert categorical columns to category dtype\n",
    "for col in categorical_cols:\n",
    "    X_train[col] = X_train[col].astype(\"category\")\n",
    "    X_test[col] = X_test[col].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dfa258-14ef-4f54-a7c6-544079d6fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target variable (y)\n",
    "# target_column = \"TotalExpense\"  # Replace with actual target column name\n",
    "# X = d_train.drop(columns=[target_column])\n",
    "# y =log_test\n",
    "\n",
    "X_train = d_train\n",
    "y_train = log_train\n",
    "X_test = d_test\n",
    "y_test = log_test\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"XGBoost\": xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {\"RMSE\": rmse, \"R² Score\": r2}\n",
    "    print(f\"{name} → RMSE: {rmse:.4f}, R² Score: {r2:.4f}\")\n",
    "    \n",
    "# Calculate Metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "# Print Results\n",
    "print(f\"✅ XGBoost R² Score: {r2:.4f}\")\n",
    "print(f\"✅ XGBoost RMSE: {rmse:.4f}\")\n",
    "print(f\"✅ XGBoost MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Stacking Model (Combining all models)\n",
    "stacking = StackingRegressor(\n",
    "    estimators=[(name, models[name]) for name in models],\n",
    "    final_estimator=Ridge(alpha=1.0)\n",
    ")\n",
    "stacking.fit(X_train, y_train)\n",
    "y_pred_stacking = stacking.predict(X_test)\n",
    "rmse_stacking = mean_squared_error(y_test, y_pred_stacking, squared=False)\n",
    "r2_stacking = r2_score(y_test, y_pred_stacking)\n",
    "\n",
    "print(f\"Stacking Model → RMSE: {rmse_stacking:.4f}, R² Score: {r2_stacking:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31049e3f-2a5e-4c11-902e-26ed028fc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = log_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "077476d0-0bee-4341-821f-621a5896f58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         8.377477\n",
       "1         9.443923\n",
       "2         9.380547\n",
       "3        10.135567\n",
       "4         9.553327\n",
       "           ...    \n",
       "52345     9.271024\n",
       "52346     9.601100\n",
       "52347    10.355267\n",
       "52348     9.280217\n",
       "52349     9.607088\n",
       "Name: log_totalexpense, Length: 52350, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5801e733-008b-4590-a638-08326381a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = d_train.drop(columns=[\"TotalExpense\", \"HH_ID\"])  # Exclude TotalExpense & HH_ID\n",
    "X_test = d_test.drop(columns=[\"TotalExpense\", \"HH_ID\"])    # Exclude from test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "313f62ac-257d-4e73-baf8-213faba93173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HH Size (For FDQ)', 'Male_Count', 'Female_Count', 'Other_Count',\n",
       "       'Age_0_18', 'Age_18_60', 'Age_60_above',\n",
       "       'Highest educational level attained_head',\n",
       "       'Total year of education completed_head',\n",
       "       'Highest educational level attained_median',\n",
       "       ...\n",
       "       'Total year of education completed_head.1',\n",
       "       'Highest educational level attained_median.1',\n",
       "       'Total year of education completed_median.1',\n",
       "       'No. of days stayed away from home during last 30 days_avg.1',\n",
       "       'No. of meals usually taken in a day_avg.1',\n",
       "       'No. of meals taken during last 30 days from school, balwadi etc._avg.1',\n",
       "       'No. of meals taken during last 30 days from employer as perquisites or part of wage_avg.1',\n",
       "       'No. of meals taken during last 30 days others_avg.1',\n",
       "       'No. of meals taken during last 30 days on payment_avg.1',\n",
       "       'No. of meals taken during last 30 days at home_avg.1'],\n",
       "      dtype='object', length=426)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cdab8716-3f49-4cce-8701-cde0dff5174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop HH_ID from training and testing feature sets\n",
    "X_train = X_train.drop(columns=[\"HH_ID\"], errors=\"ignore\")\n",
    "X_test = X_test.drop(columns=[\"HH_ID\"], errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27dd8c2b-7198-4194-aa4a-32470fb33439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree on Log-Transformed Target → RMSE: 10465.6369, R² Score: 0.4116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize Decision Tree Regressor\n",
    "dt_model = DecisionTreeRegressor(max_depth=6, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_log = dt_model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "y_pred_original = np.expm1(y_pred_log)\n",
    "y_test_original = np.expm1(y_test)\n",
    "\n",
    "# Evaluate Model Performance\n",
    "rmse = mean_squared_error(y_test_original, y_pred_original, squared=False)\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "print(f\"Decision Tree on Log-Transformed Target → RMSE: {rmse:.4f}, R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0eb1622e-6789-4095-bf8f-643498383e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1099345-e3a1-4f94-810b-5fbb44a539eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix column names for LightGBM (replace special characters with \"_\")\n",
    "X_train.columns = X_train.columns.str.replace('[^a-zA-Z0-9]', '_', regex=True)\n",
    "X_test.columns = X_test.columns.str.replace('[^a-zA-Z0-9]', '_', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc7c0502-a9cb-4b5f-828a-8eb717a4dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names by replacing special characters\n",
    "d_train.columns = d_train.columns.str.replace('[^a-zA-Z0-9]', '_', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d3bd2e1-2389-409c-8309-ac2491a58ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names by replacing special characters\n",
    "d_test.columns = d_test.columns.str.replace('[^a-zA-Z0-9]', '_', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6833cbaa-7391-49ab-8ada-2efc51476902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1e3f721-c6c6-4f21-8c19-a1f0db9c0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2b9db1f-9522-4187-9a4c-9b4c683afabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 4ms/step - loss: 0.7011 - mae: 0.4182 - val_loss: 0.1673 - val_mae: 0.3306\n",
      "Epoch 2/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.1238 - mae: 0.2716 - val_loss: 0.1081 - val_mae: 0.2516\n",
      "Epoch 3/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - loss: 0.1164 - mae: 0.2627 - val_loss: 0.1115 - val_mae: 0.2531\n",
      "Epoch 4/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 4ms/step - loss: 0.1122 - mae: 0.2573 - val_loss: 0.1102 - val_mae: 0.2515\n",
      "Epoch 5/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - loss: 0.1099 - mae: 0.2545 - val_loss: 0.1201 - val_mae: 0.2636\n",
      "Epoch 6/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 4ms/step - loss: 0.1075 - mae: 0.2517 - val_loss: 0.1044 - val_mae: 0.2454\n",
      "Epoch 7/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - loss: 0.1064 - mae: 0.2508 - val_loss: 0.1026 - val_mae: 0.2436\n",
      "Epoch 8/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3ms/step - loss: 0.1048 - mae: 0.2488 - val_loss: 0.1016 - val_mae: 0.2424\n",
      "Epoch 9/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4ms/step - loss: 0.1032 - mae: 0.2467 - val_loss: 0.1008 - val_mae: 0.2412\n",
      "Epoch 10/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 3ms/step - loss: 0.1028 - mae: 0.2458 - val_loss: 0.1035 - val_mae: 0.2467\n",
      "Epoch 11/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 4ms/step - loss: 0.1021 - mae: 0.2450 - val_loss: 0.1048 - val_mae: 0.2489\n",
      "Epoch 12/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 4ms/step - loss: 0.1012 - mae: 0.2437 - val_loss: 0.1001 - val_mae: 0.2416\n",
      "Epoch 13/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 3ms/step - loss: 0.1009 - mae: 0.2438 - val_loss: 0.1119 - val_mae: 0.2543\n",
      "Epoch 14/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 4ms/step - loss: 0.0997 - mae: 0.2416 - val_loss: 0.1007 - val_mae: 0.2406\n",
      "Epoch 15/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 4ms/step - loss: 0.0997 - mae: 0.2417 - val_loss: 0.1004 - val_mae: 0.2411\n",
      "Epoch 16/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 4ms/step - loss: 0.0987 - mae: 0.2408 - val_loss: 0.1016 - val_mae: 0.2405\n",
      "Epoch 17/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0976 - mae: 0.2391 - val_loss: 0.0988 - val_mae: 0.2397\n",
      "Epoch 18/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 4ms/step - loss: 0.0981 - mae: 0.2398 - val_loss: 0.0983 - val_mae: 0.2381\n",
      "Epoch 19/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4ms/step - loss: 0.0976 - mae: 0.2390 - val_loss: 0.1077 - val_mae: 0.2483\n",
      "Epoch 20/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 4ms/step - loss: 0.0975 - mae: 0.2381 - val_loss: 0.0984 - val_mae: 0.2380\n",
      "Epoch 21/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4ms/step - loss: 0.0965 - mae: 0.2377 - val_loss: 0.1002 - val_mae: 0.2394\n",
      "Epoch 22/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 4ms/step - loss: 0.0962 - mae: 0.2375 - val_loss: 0.1013 - val_mae: 0.2405\n",
      "Epoch 23/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0963 - mae: 0.2371 - val_loss: 0.0996 - val_mae: 0.2413\n",
      "Epoch 24/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3ms/step - loss: 0.0942 - mae: 0.2348 - val_loss: 0.0990 - val_mae: 0.2378\n",
      "Epoch 25/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 4ms/step - loss: 0.0947 - mae: 0.2354 - val_loss: 0.1006 - val_mae: 0.2435\n",
      "Epoch 26/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4ms/step - loss: 0.0958 - mae: 0.2366 - val_loss: 0.1022 - val_mae: 0.2460\n",
      "Epoch 27/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 5ms/step - loss: 0.0944 - mae: 0.2346 - val_loss: 0.0981 - val_mae: 0.2371\n",
      "Epoch 28/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4ms/step - loss: 0.0945 - mae: 0.2351 - val_loss: 0.0994 - val_mae: 0.2388\n",
      "Epoch 29/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 4ms/step - loss: 0.0946 - mae: 0.2352 - val_loss: 0.0975 - val_mae: 0.2363\n",
      "Epoch 30/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0949 - mae: 0.2354 - val_loss: 0.0965 - val_mae: 0.2357\n",
      "Epoch 31/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 4ms/step - loss: 0.0944 - mae: 0.2347 - val_loss: 0.0989 - val_mae: 0.2380\n",
      "Epoch 32/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 4ms/step - loss: 0.0941 - mae: 0.2339 - val_loss: 0.1088 - val_mae: 0.2556\n",
      "Epoch 33/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 4ms/step - loss: 0.0932 - mae: 0.2336 - val_loss: 0.0973 - val_mae: 0.2368\n",
      "Epoch 34/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4ms/step - loss: 0.0937 - mae: 0.2339 - val_loss: 0.0999 - val_mae: 0.2385\n",
      "Epoch 35/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - loss: 0.0931 - mae: 0.2328 - val_loss: 0.0972 - val_mae: 0.2366\n",
      "Epoch 36/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 5ms/step - loss: 0.0929 - mae: 0.2326 - val_loss: 0.0962 - val_mae: 0.2350\n",
      "Epoch 37/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0924 - mae: 0.2325 - val_loss: 0.0992 - val_mae: 0.2379\n",
      "Epoch 38/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 0.0926 - mae: 0.2318 - val_loss: 0.1041 - val_mae: 0.2487\n",
      "Epoch 39/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 0.0925 - mae: 0.2320 - val_loss: 0.0966 - val_mae: 0.2361\n",
      "Epoch 40/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 5ms/step - loss: 0.0924 - mae: 0.2320 - val_loss: 0.0986 - val_mae: 0.2404\n",
      "Epoch 41/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0917 - mae: 0.2314 - val_loss: 0.1018 - val_mae: 0.2454\n",
      "Epoch 42/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0925 - mae: 0.2323 - val_loss: 0.0961 - val_mae: 0.2346\n",
      "Epoch 43/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 5ms/step - loss: 0.0913 - mae: 0.2306 - val_loss: 0.0968 - val_mae: 0.2355\n",
      "Epoch 44/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 6ms/step - loss: 0.0915 - mae: 0.2311 - val_loss: 0.0973 - val_mae: 0.2353\n",
      "Epoch 45/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 0.0914 - mae: 0.2314 - val_loss: 0.1009 - val_mae: 0.2404\n",
      "Epoch 46/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 6ms/step - loss: 0.0915 - mae: 0.2310 - val_loss: 0.0960 - val_mae: 0.2350\n",
      "Epoch 47/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5ms/step - loss: 0.0906 - mae: 0.2303 - val_loss: 0.0957 - val_mae: 0.2341\n",
      "Epoch 48/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5ms/step - loss: 0.0909 - mae: 0.2298 - val_loss: 0.1004 - val_mae: 0.2432\n",
      "Epoch 49/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 0.0910 - mae: 0.2299 - val_loss: 0.0985 - val_mae: 0.2397\n",
      "Epoch 50/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 5ms/step - loss: 0.0906 - mae: 0.2299 - val_loss: 0.0977 - val_mae: 0.2357\n",
      "Epoch 51/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0908 - mae: 0.2301 - val_loss: 0.0961 - val_mae: 0.2354\n",
      "Epoch 52/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0908 - mae: 0.2298 - val_loss: 0.0955 - val_mae: 0.2345\n",
      "Epoch 53/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5ms/step - loss: 0.0909 - mae: 0.2303 - val_loss: 0.0957 - val_mae: 0.2340\n",
      "Epoch 54/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 5ms/step - loss: 0.0906 - mae: 0.2298 - val_loss: 0.0977 - val_mae: 0.2385\n",
      "Epoch 55/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0902 - mae: 0.2288 - val_loss: 0.1033 - val_mae: 0.2477\n",
      "Epoch 56/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - loss: 0.0914 - mae: 0.2302 - val_loss: 0.0961 - val_mae: 0.2346\n",
      "Epoch 57/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 5ms/step - loss: 0.0898 - mae: 0.2290 - val_loss: 0.1005 - val_mae: 0.2433\n",
      "Epoch 58/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0898 - mae: 0.2289 - val_loss: 0.0970 - val_mae: 0.2352\n",
      "Epoch 59/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0906 - mae: 0.2292 - val_loss: 0.1060 - val_mae: 0.2518\n",
      "Epoch 60/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0910 - mae: 0.2302 - val_loss: 0.1015 - val_mae: 0.2447\n",
      "Epoch 61/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5ms/step - loss: 0.0904 - mae: 0.2291 - val_loss: 0.0958 - val_mae: 0.2336\n",
      "Epoch 62/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 0.0897 - mae: 0.2287 - val_loss: 0.0954 - val_mae: 0.2337\n",
      "Epoch 63/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 5ms/step - loss: 0.0906 - mae: 0.2298 - val_loss: 0.1046 - val_mae: 0.2500\n",
      "Epoch 64/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0891 - mae: 0.2279 - val_loss: 0.0960 - val_mae: 0.2344\n",
      "Epoch 65/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 6ms/step - loss: 0.0904 - mae: 0.2290 - val_loss: 0.0957 - val_mae: 0.2342\n",
      "Epoch 66/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 5ms/step - loss: 0.0899 - mae: 0.2287 - val_loss: 0.0970 - val_mae: 0.2376\n",
      "Epoch 67/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 5ms/step - loss: 0.0901 - mae: 0.2291 - val_loss: 0.0971 - val_mae: 0.2357\n",
      "Epoch 68/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 5ms/step - loss: 0.0899 - mae: 0.2288 - val_loss: 0.0958 - val_mae: 0.2342\n",
      "Epoch 69/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 5ms/step - loss: 0.0899 - mae: 0.2283 - val_loss: 0.0996 - val_mae: 0.2412\n",
      "Epoch 70/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 0.0896 - mae: 0.2281 - val_loss: 0.0963 - val_mae: 0.2363\n",
      "Epoch 71/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5ms/step - loss: 0.0899 - mae: 0.2284 - val_loss: 0.0973 - val_mae: 0.2384\n",
      "Epoch 72/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5ms/step - loss: 0.0888 - mae: 0.2275 - val_loss: 0.0977 - val_mae: 0.2360\n",
      "Epoch 73/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 5ms/step - loss: 0.0895 - mae: 0.2284 - val_loss: 0.0977 - val_mae: 0.2358\n",
      "Epoch 74/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 5ms/step - loss: 0.0894 - mae: 0.2276 - val_loss: 0.0968 - val_mae: 0.2350\n",
      "Epoch 75/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 6ms/step - loss: 0.0892 - mae: 0.2275 - val_loss: 0.0958 - val_mae: 0.2343\n",
      "Epoch 76/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 5ms/step - loss: 0.0895 - mae: 0.2279 - val_loss: 0.0973 - val_mae: 0.2383\n",
      "Epoch 77/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5ms/step - loss: 0.0888 - mae: 0.2272 - val_loss: 0.1003 - val_mae: 0.2387\n",
      "Epoch 78/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 0.0884 - mae: 0.2267 - val_loss: 0.0960 - val_mae: 0.2354\n",
      "Epoch 79/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5ms/step - loss: 0.0890 - mae: 0.2274 - val_loss: 0.1007 - val_mae: 0.2441\n",
      "Epoch 80/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5ms/step - loss: 0.0889 - mae: 0.2270 - val_loss: 0.0972 - val_mae: 0.2378\n",
      "Epoch 81/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5ms/step - loss: 0.0888 - mae: 0.2272 - val_loss: 0.0970 - val_mae: 0.2372\n",
      "Epoch 82/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5ms/step - loss: 0.0883 - mae: 0.2267 - val_loss: 0.0973 - val_mae: 0.2377\n",
      "Epoch 83/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5ms/step - loss: 0.0887 - mae: 0.2269 - val_loss: 0.1006 - val_mae: 0.2397\n",
      "Epoch 84/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0892 - mae: 0.2274 - val_loss: 0.0958 - val_mae: 0.2342\n",
      "Epoch 85/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 6ms/step - loss: 0.0891 - mae: 0.2272 - val_loss: 0.0962 - val_mae: 0.2345\n",
      "Epoch 86/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 6ms/step - loss: 0.0885 - mae: 0.2269 - val_loss: 0.0991 - val_mae: 0.2374\n",
      "Epoch 87/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 9ms/step - loss: 0.0884 - mae: 0.2267 - val_loss: 0.0962 - val_mae: 0.2341\n",
      "Epoch 88/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 7ms/step - loss: 0.0894 - mae: 0.2271 - val_loss: 0.1000 - val_mae: 0.2385\n",
      "Epoch 89/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 5ms/step - loss: 0.0886 - mae: 0.2267 - val_loss: 0.0967 - val_mae: 0.2350\n",
      "Epoch 90/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 5ms/step - loss: 0.0887 - mae: 0.2268 - val_loss: 0.0955 - val_mae: 0.2341\n",
      "Epoch 91/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - loss: 0.0880 - mae: 0.2259 - val_loss: 0.0957 - val_mae: 0.2338\n",
      "Epoch 92/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 5ms/step - loss: 0.0885 - mae: 0.2269 - val_loss: 0.0965 - val_mae: 0.2351\n",
      "Epoch 93/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 5ms/step - loss: 0.0881 - mae: 0.2264 - val_loss: 0.0954 - val_mae: 0.2340\n",
      "Epoch 94/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 6ms/step - loss: 0.0885 - mae: 0.2263 - val_loss: 0.0994 - val_mae: 0.2378\n",
      "Epoch 95/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 5ms/step - loss: 0.0878 - mae: 0.2258 - val_loss: 0.0963 - val_mae: 0.2344\n",
      "Epoch 96/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 5ms/step - loss: 0.0891 - mae: 0.2272 - val_loss: 0.1060 - val_mae: 0.2458\n",
      "Epoch 97/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 5ms/step - loss: 0.0890 - mae: 0.2272 - val_loss: 0.0973 - val_mae: 0.2379\n",
      "Epoch 98/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0881 - mae: 0.2258 - val_loss: 0.0996 - val_mae: 0.2416\n",
      "Epoch 99/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5ms/step - loss: 0.0888 - mae: 0.2269 - val_loss: 0.1027 - val_mae: 0.2470\n",
      "Epoch 100/100\n",
      "\u001b[1m6544/6544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 5ms/step - loss: 0.0884 - mae: 0.2264 - val_loss: 0.0974 - val_mae: 0.2355\n"
     ]
    }
   ],
   "source": [
    "# Define ANN model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fbe987e-e7f1-4daf-be14-0e9836e56e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1636/1636\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "ANN Model → RMSE: 9027.2757, R² Score: 0.5622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred_log = model.predict(X_test).flatten()\n",
    "\n",
    "# Convert log predictions back to original scale\n",
    "y_pred_original = np.expm1(y_pred_log)\n",
    "y_test_original = np.expm1(y_test)\n",
    "\n",
    "# Evaluate model\n",
    "rmse = mean_squared_error(y_test_original, y_pred_original, squared=False)\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "print(f\"ANN Model → RMSE: {rmse:.4f}, R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab73dad3-4c74-44b9-b0a2-8d3e783795ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 10ms/step - loss: 1.6252 - mae: 0.8682 - val_loss: 8.7700 - val_mae: 2.9370\n",
      "Epoch 2/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 13ms/step - loss: 0.2662 - mae: 0.4031 - val_loss: 2.8133 - val_mae: 1.6316\n",
      "Epoch 3/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 9ms/step - loss: 0.1538 - mae: 0.3047 - val_loss: 0.4093 - val_mae: 0.5514\n",
      "Epoch 4/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 8ms/step - loss: 0.1247 - mae: 0.2716 - val_loss: 0.1589 - val_mae: 0.3092\n",
      "Epoch 5/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 8ms/step - loss: 0.1192 - mae: 0.2656 - val_loss: 0.1540 - val_mae: 0.3049\n",
      "Epoch 6/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1164 - mae: 0.2618 - val_loss: 0.1460 - val_mae: 0.2944\n",
      "Epoch 7/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1154 - mae: 0.2605 - val_loss: 0.1405 - val_mae: 0.2892\n",
      "Epoch 8/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.1144 - mae: 0.2595 - val_loss: 0.1302 - val_mae: 0.2758\n",
      "Epoch 9/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 0.1139 - mae: 0.2586 - val_loss: 0.1349 - val_mae: 0.2824\n",
      "Epoch 10/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 14ms/step - loss: 0.1120 - mae: 0.2567 - val_loss: 0.1585 - val_mae: 0.3100\n",
      "Epoch 11/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1122 - mae: 0.2567 - val_loss: 0.1460 - val_mae: 0.2960\n",
      "Epoch 12/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.1106 - mae: 0.2551 - val_loss: 0.1402 - val_mae: 0.2882\n",
      "Epoch 13/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.1114 - mae: 0.2558 - val_loss: 0.1636 - val_mae: 0.3147\n",
      "Epoch 14/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.1105 - mae: 0.2546 - val_loss: 0.1443 - val_mae: 0.2931\n",
      "Epoch 15/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1090 - mae: 0.2528 - val_loss: 0.1214 - val_mae: 0.2658\n",
      "Epoch 16/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.1093 - mae: 0.2533 - val_loss: 0.1285 - val_mae: 0.2739\n",
      "Epoch 17/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1086 - mae: 0.2525 - val_loss: 0.1300 - val_mae: 0.2750\n",
      "Epoch 18/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1081 - mae: 0.2516 - val_loss: 0.1345 - val_mae: 0.2812\n",
      "Epoch 19/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8ms/step - loss: 0.1080 - mae: 0.2520 - val_loss: 0.1270 - val_mae: 0.2707\n",
      "Epoch 20/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 8ms/step - loss: 0.1076 - mae: 0.2512 - val_loss: 0.1231 - val_mae: 0.2661\n",
      "Epoch 21/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 0.1080 - mae: 0.2514 - val_loss: 0.1314 - val_mae: 0.2767\n",
      "Epoch 22/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 0.1065 - mae: 0.2499 - val_loss: 0.1222 - val_mae: 0.2656\n",
      "Epoch 23/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 7ms/step - loss: 0.1077 - mae: 0.2510 - val_loss: 0.1306 - val_mae: 0.2763\n",
      "Epoch 24/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.1061 - mae: 0.2500 - val_loss: 0.1419 - val_mae: 0.2902\n",
      "Epoch 25/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1063 - mae: 0.2495 - val_loss: 0.1312 - val_mae: 0.2757\n",
      "Epoch 26/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - loss: 0.1055 - mae: 0.2484 - val_loss: 0.1346 - val_mae: 0.2803\n",
      "Epoch 27/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1050 - mae: 0.2479 - val_loss: 0.1235 - val_mae: 0.2672\n",
      "Epoch 28/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.1059 - mae: 0.2487 - val_loss: 0.1251 - val_mae: 0.2684\n",
      "Epoch 29/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.1042 - mae: 0.2472 - val_loss: 0.1182 - val_mae: 0.2613\n",
      "Epoch 30/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.1045 - mae: 0.2472 - val_loss: 0.1210 - val_mae: 0.2639\n",
      "Epoch 31/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.1042 - mae: 0.2464 - val_loss: 0.1174 - val_mae: 0.2607\n",
      "Epoch 32/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.1036 - mae: 0.2459 - val_loss: 0.1405 - val_mae: 0.2885\n",
      "Epoch 33/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1027 - mae: 0.2453 - val_loss: 0.1347 - val_mae: 0.2811\n",
      "Epoch 34/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 8ms/step - loss: 0.1029 - mae: 0.2449 - val_loss: 0.1247 - val_mae: 0.2683\n",
      "Epoch 35/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.1028 - mae: 0.2458 - val_loss: 0.1199 - val_mae: 0.2630\n",
      "Epoch 36/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.1041 - mae: 0.2458 - val_loss: 0.1190 - val_mae: 0.2616\n",
      "Epoch 37/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1029 - mae: 0.2452 - val_loss: 0.1119 - val_mae: 0.2540\n",
      "Epoch 38/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1028 - mae: 0.2447 - val_loss: 0.1155 - val_mae: 0.2583\n",
      "Epoch 39/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 8ms/step - loss: 0.1022 - mae: 0.2443 - val_loss: 0.1148 - val_mae: 0.2571\n",
      "Epoch 40/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.1028 - mae: 0.2452 - val_loss: 0.1157 - val_mae: 0.2589\n",
      "Epoch 41/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 8ms/step - loss: 0.1033 - mae: 0.2456 - val_loss: 0.1251 - val_mae: 0.2682\n",
      "Epoch 42/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1023 - mae: 0.2444 - val_loss: 0.1167 - val_mae: 0.2589\n",
      "Epoch 43/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 7ms/step - loss: 0.1025 - mae: 0.2441 - val_loss: 0.1123 - val_mae: 0.2546\n",
      "Epoch 44/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1011 - mae: 0.2429 - val_loss: 0.1245 - val_mae: 0.2694\n",
      "Epoch 45/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8ms/step - loss: 0.1018 - mae: 0.2438 - val_loss: 0.1177 - val_mae: 0.2611\n",
      "Epoch 46/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.1019 - mae: 0.2443 - val_loss: 0.1134 - val_mae: 0.2558\n",
      "Epoch 47/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 7ms/step - loss: 0.1022 - mae: 0.2446 - val_loss: 0.1133 - val_mae: 0.2545\n",
      "Epoch 48/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1018 - mae: 0.2435 - val_loss: 0.1178 - val_mae: 0.2613\n",
      "Epoch 49/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.1022 - mae: 0.2440 - val_loss: 0.1159 - val_mae: 0.2591\n",
      "Epoch 50/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 0.1010 - mae: 0.2429 - val_loss: 0.1121 - val_mae: 0.2546\n",
      "Epoch 51/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - loss: 0.1012 - mae: 0.2429 - val_loss: 0.1143 - val_mae: 0.2568\n",
      "Epoch 52/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1007 - mae: 0.2427 - val_loss: 0.1113 - val_mae: 0.2538\n",
      "Epoch 53/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 16ms/step - loss: 0.1013 - mae: 0.2432 - val_loss: 0.1108 - val_mae: 0.2527\n",
      "Epoch 54/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1011 - mae: 0.2430 - val_loss: 0.1155 - val_mae: 0.2582\n",
      "Epoch 55/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.1003 - mae: 0.2422 - val_loss: 0.1133 - val_mae: 0.2555\n",
      "Epoch 56/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 14ms/step - loss: 0.1002 - mae: 0.2417 - val_loss: 0.1134 - val_mae: 0.2559\n",
      "Epoch 57/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 8ms/step - loss: 0.1001 - mae: 0.2418 - val_loss: 0.1137 - val_mae: 0.2571\n",
      "Epoch 58/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.0999 - mae: 0.2420 - val_loss: 0.1169 - val_mae: 0.2599\n",
      "Epoch 59/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.1008 - mae: 0.2424 - val_loss: 0.1102 - val_mae: 0.2521\n",
      "Epoch 60/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1008 - mae: 0.2426 - val_loss: 0.1131 - val_mae: 0.2552\n",
      "Epoch 61/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.1010 - mae: 0.2426 - val_loss: 0.1131 - val_mae: 0.2555\n",
      "Epoch 62/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.1011 - mae: 0.2431 - val_loss: 0.1137 - val_mae: 0.2565\n",
      "Epoch 63/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 13ms/step - loss: 0.0997 - mae: 0.2411 - val_loss: 0.1242 - val_mae: 0.2681\n",
      "Epoch 64/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1005 - mae: 0.2417 - val_loss: 0.1166 - val_mae: 0.2601\n",
      "Epoch 65/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.0994 - mae: 0.2412 - val_loss: 0.1151 - val_mae: 0.2584\n",
      "Epoch 66/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.0998 - mae: 0.2413 - val_loss: 0.1121 - val_mae: 0.2549\n",
      "Epoch 67/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.1000 - mae: 0.2420 - val_loss: 0.1142 - val_mae: 0.2570\n",
      "Epoch 68/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.0995 - mae: 0.2406 - val_loss: 0.1143 - val_mae: 0.2568\n",
      "Epoch 69/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.0995 - mae: 0.2414 - val_loss: 0.1091 - val_mae: 0.2503\n",
      "Epoch 70/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.0986 - mae: 0.2401 - val_loss: 0.1142 - val_mae: 0.2566\n",
      "Epoch 71/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11ms/step - loss: 0.0993 - mae: 0.2409 - val_loss: 0.1158 - val_mae: 0.2577\n",
      "Epoch 72/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0987 - mae: 0.2399 - val_loss: 0.1105 - val_mae: 0.2523\n",
      "Epoch 73/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 7ms/step - loss: 0.0993 - mae: 0.2408 - val_loss: 0.1077 - val_mae: 0.2499\n",
      "Epoch 74/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.0979 - mae: 0.2394 - val_loss: 0.1082 - val_mae: 0.2500\n",
      "Epoch 75/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.0992 - mae: 0.2408 - val_loss: 0.1115 - val_mae: 0.2535\n",
      "Epoch 76/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.0986 - mae: 0.2401 - val_loss: 0.1150 - val_mae: 0.2583\n",
      "Epoch 77/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.0985 - mae: 0.2402 - val_loss: 0.1119 - val_mae: 0.2543\n",
      "Epoch 78/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - loss: 0.0980 - mae: 0.2392 - val_loss: 0.1115 - val_mae: 0.2534\n",
      "Epoch 79/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.0988 - mae: 0.2400 - val_loss: 0.1086 - val_mae: 0.2498\n",
      "Epoch 80/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - loss: 0.0992 - mae: 0.2410 - val_loss: 0.1138 - val_mae: 0.2563\n",
      "Epoch 81/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.0986 - mae: 0.2397 - val_loss: 0.1172 - val_mae: 0.2607\n",
      "Epoch 82/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 8ms/step - loss: 0.0985 - mae: 0.2397 - val_loss: 0.1145 - val_mae: 0.2571\n",
      "Epoch 83/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - loss: 0.0988 - mae: 0.2399 - val_loss: 0.1062 - val_mae: 0.2472\n",
      "Epoch 84/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.0977 - mae: 0.2391 - val_loss: 0.1070 - val_mae: 0.2483\n",
      "Epoch 85/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 0.0969 - mae: 0.2381 - val_loss: 0.1082 - val_mae: 0.2497\n",
      "Epoch 86/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 19ms/step - loss: 0.0983 - mae: 0.2391 - val_loss: 0.1047 - val_mae: 0.2456\n",
      "Epoch 87/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 0.0982 - mae: 0.2392 - val_loss: 0.1050 - val_mae: 0.2463\n",
      "Epoch 88/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0979 - mae: 0.2386 - val_loss: 0.1119 - val_mae: 0.2537\n",
      "Epoch 89/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 9ms/step - loss: 0.0978 - mae: 0.2387 - val_loss: 0.1047 - val_mae: 0.2452\n",
      "Epoch 90/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0977 - mae: 0.2393 - val_loss: 0.1043 - val_mae: 0.2449\n",
      "Epoch 91/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0976 - mae: 0.2390 - val_loss: 0.1189 - val_mae: 0.2629\n",
      "Epoch 92/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0977 - mae: 0.2391 - val_loss: 0.1099 - val_mae: 0.2517\n",
      "Epoch 93/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0963 - mae: 0.2369 - val_loss: 0.1082 - val_mae: 0.2496\n",
      "Epoch 94/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0974 - mae: 0.2381 - val_loss: 0.1104 - val_mae: 0.2518\n",
      "Epoch 95/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 13ms/step - loss: 0.0974 - mae: 0.2387 - val_loss: 0.1057 - val_mae: 0.2460\n",
      "Epoch 96/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0976 - mae: 0.2388 - val_loss: 0.1112 - val_mae: 0.2533\n",
      "Epoch 97/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0973 - mae: 0.2383 - val_loss: 0.1045 - val_mae: 0.2455\n",
      "Epoch 98/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0963 - mae: 0.2376 - val_loss: 0.1063 - val_mae: 0.2477\n",
      "Epoch 99/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0981 - mae: 0.2390 - val_loss: 0.1153 - val_mae: 0.2577\n",
      "Epoch 100/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10ms/step - loss: 0.0979 - mae: 0.2391 - val_loss: 0.1108 - val_mae: 0.2527\n",
      "Epoch 101/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 14ms/step - loss: 0.0962 - mae: 0.2374 - val_loss: 0.1026 - val_mae: 0.2430\n",
      "Epoch 102/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11ms/step - loss: 0.0973 - mae: 0.2380 - val_loss: 0.1090 - val_mae: 0.2515\n",
      "Epoch 103/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 10ms/step - loss: 0.0974 - mae: 0.2385 - val_loss: 0.1114 - val_mae: 0.2539\n",
      "Epoch 104/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0969 - mae: 0.2379 - val_loss: 0.1051 - val_mae: 0.2459\n",
      "Epoch 105/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0960 - mae: 0.2370 - val_loss: 0.1091 - val_mae: 0.2513\n",
      "Epoch 106/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0973 - mae: 0.2382 - val_loss: 0.1085 - val_mae: 0.2505\n",
      "Epoch 107/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 10ms/step - loss: 0.0959 - mae: 0.2367 - val_loss: 0.1085 - val_mae: 0.2498\n",
      "Epoch 108/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0963 - mae: 0.2367 - val_loss: 0.1125 - val_mae: 0.2548\n",
      "Epoch 109/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 0.0982 - mae: 0.2393 - val_loss: 0.1155 - val_mae: 0.2585\n",
      "Epoch 110/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11ms/step - loss: 0.0968 - mae: 0.2376 - val_loss: 0.1089 - val_mae: 0.2505\n",
      "Epoch 111/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 10ms/step - loss: 0.0969 - mae: 0.2382 - val_loss: 0.1036 - val_mae: 0.2442\n",
      "Epoch 112/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0973 - mae: 0.2381 - val_loss: 0.1106 - val_mae: 0.2527\n",
      "Epoch 113/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 11ms/step - loss: 0.0964 - mae: 0.2374 - val_loss: 0.1103 - val_mae: 0.2526\n",
      "Epoch 114/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0956 - mae: 0.2364 - val_loss: 0.1011 - val_mae: 0.2414\n",
      "Epoch 115/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0962 - mae: 0.2371 - val_loss: 0.1127 - val_mae: 0.2549\n",
      "Epoch 116/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 10ms/step - loss: 0.0969 - mae: 0.2371 - val_loss: 0.1060 - val_mae: 0.2472\n",
      "Epoch 117/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0958 - mae: 0.2365 - val_loss: 0.1084 - val_mae: 0.2500\n",
      "Epoch 118/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 0.0973 - mae: 0.2378 - val_loss: 0.1109 - val_mae: 0.2531\n",
      "Epoch 119/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0962 - mae: 0.2368 - val_loss: 0.1095 - val_mae: 0.2511\n",
      "Epoch 120/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10ms/step - loss: 0.0956 - mae: 0.2360 - val_loss: 0.1145 - val_mae: 0.2572\n",
      "Epoch 121/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 0.0956 - mae: 0.2366 - val_loss: 0.1099 - val_mae: 0.2515\n",
      "Epoch 122/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 11ms/step - loss: 0.0962 - mae: 0.2365 - val_loss: 0.1062 - val_mae: 0.2476\n",
      "Epoch 123/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10ms/step - loss: 0.0959 - mae: 0.2369 - val_loss: 0.1069 - val_mae: 0.2479\n",
      "Epoch 124/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 0.0966 - mae: 0.2374 - val_loss: 0.1026 - val_mae: 0.2432\n",
      "Epoch 125/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0960 - mae: 0.2364 - val_loss: 0.1084 - val_mae: 0.2501\n",
      "Epoch 126/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0966 - mae: 0.2371 - val_loss: 0.1067 - val_mae: 0.2482\n",
      "Epoch 127/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0951 - mae: 0.2356 - val_loss: 0.1097 - val_mae: 0.2515\n",
      "Epoch 128/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 0.0957 - mae: 0.2356 - val_loss: 0.1142 - val_mae: 0.2572\n",
      "Epoch 129/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 10ms/step - loss: 0.0957 - mae: 0.2363 - val_loss: 0.1026 - val_mae: 0.2425\n",
      "Epoch 130/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 0.0952 - mae: 0.2356 - val_loss: 0.1039 - val_mae: 0.2438\n",
      "Epoch 131/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - loss: 0.0953 - mae: 0.2358 - val_loss: 0.1075 - val_mae: 0.2482\n",
      "Epoch 132/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0962 - mae: 0.2367 - val_loss: 0.1066 - val_mae: 0.2477\n",
      "Epoch 133/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 10ms/step - loss: 0.0954 - mae: 0.2360 - val_loss: 0.1053 - val_mae: 0.2458\n",
      "Epoch 134/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 11ms/step - loss: 0.0955 - mae: 0.2358 - val_loss: 0.1060 - val_mae: 0.2470\n",
      "Epoch 135/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 0.0960 - mae: 0.2367 - val_loss: 0.1083 - val_mae: 0.2497\n",
      "Epoch 136/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 13ms/step - loss: 0.0954 - mae: 0.2358 - val_loss: 0.1072 - val_mae: 0.2496\n",
      "Epoch 137/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 18ms/step - loss: 0.0953 - mae: 0.2360 - val_loss: 0.1100 - val_mae: 0.2522\n",
      "Epoch 138/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 11ms/step - loss: 0.0959 - mae: 0.2364 - val_loss: 0.1046 - val_mae: 0.2454\n",
      "Epoch 139/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 10ms/step - loss: 0.0959 - mae: 0.2361 - val_loss: 0.1090 - val_mae: 0.2511\n",
      "Epoch 140/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 17ms/step - loss: 0.0955 - mae: 0.2358 - val_loss: 0.1108 - val_mae: 0.2528\n",
      "Epoch 141/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 0.0950 - mae: 0.2354 - val_loss: 0.1071 - val_mae: 0.2473\n",
      "Epoch 142/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.0945 - mae: 0.2350 - val_loss: 0.1031 - val_mae: 0.2428\n",
      "Epoch 143/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 8ms/step - loss: 0.0952 - mae: 0.2354 - val_loss: 0.1025 - val_mae: 0.2425\n",
      "Epoch 144/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 7ms/step - loss: 0.0952 - mae: 0.2359 - val_loss: 0.1077 - val_mae: 0.2502\n",
      "Epoch 145/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - loss: 0.0957 - mae: 0.2366 - val_loss: 0.1034 - val_mae: 0.2438\n",
      "Epoch 146/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.0948 - mae: 0.2347 - val_loss: 0.1052 - val_mae: 0.2457\n",
      "Epoch 147/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0942 - mae: 0.2346 - val_loss: 0.1050 - val_mae: 0.2466\n",
      "Epoch 148/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 0.0951 - mae: 0.2354 - val_loss: 0.1051 - val_mae: 0.2455\n",
      "Epoch 149/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9ms/step - loss: 0.0955 - mae: 0.2357 - val_loss: 0.1027 - val_mae: 0.2437\n",
      "Epoch 150/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - loss: 0.0957 - mae: 0.2360 - val_loss: 0.1035 - val_mae: 0.2440\n",
      "Epoch 151/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0957 - mae: 0.2355 - val_loss: 0.1076 - val_mae: 0.2481\n",
      "Epoch 152/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0959 - mae: 0.2365 - val_loss: 0.1006 - val_mae: 0.2407\n",
      "Epoch 153/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - loss: 0.0950 - mae: 0.2353 - val_loss: 0.1029 - val_mae: 0.2428\n",
      "Epoch 154/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11ms/step - loss: 0.0953 - mae: 0.2357 - val_loss: 0.1012 - val_mae: 0.2415\n",
      "Epoch 155/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 0.0945 - mae: 0.2344 - val_loss: 0.1060 - val_mae: 0.2462\n",
      "Epoch 156/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0946 - mae: 0.2351 - val_loss: 0.1031 - val_mae: 0.2426\n",
      "Epoch 157/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 75ms/step - loss: 0.0947 - mae: 0.2354 - val_loss: 0.1052 - val_mae: 0.2455\n",
      "Epoch 158/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0939 - mae: 0.2344 - val_loss: 0.1120 - val_mae: 0.2540\n",
      "Epoch 159/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0951 - mae: 0.2357 - val_loss: 0.1019 - val_mae: 0.2415\n",
      "Epoch 160/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9ms/step - loss: 0.0947 - mae: 0.2355 - val_loss: 0.1058 - val_mae: 0.2465\n",
      "Epoch 161/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0941 - mae: 0.2344 - val_loss: 0.1093 - val_mae: 0.2510\n",
      "Epoch 162/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0955 - mae: 0.2353 - val_loss: 0.1001 - val_mae: 0.2403\n",
      "Epoch 163/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 0.0939 - mae: 0.2342 - val_loss: 0.1038 - val_mae: 0.2437\n",
      "Epoch 164/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 0.0942 - mae: 0.2342 - val_loss: 0.1073 - val_mae: 0.2486\n",
      "Epoch 165/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 0.0941 - mae: 0.2347 - val_loss: 0.1012 - val_mae: 0.2405\n",
      "Epoch 166/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0945 - mae: 0.2349 - val_loss: 0.1019 - val_mae: 0.2418\n",
      "Epoch 167/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0949 - mae: 0.2349 - val_loss: 0.1104 - val_mae: 0.2515\n",
      "Epoch 168/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0944 - mae: 0.2345 - val_loss: 0.0994 - val_mae: 0.2400\n",
      "Epoch 169/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0937 - mae: 0.2341 - val_loss: 0.1053 - val_mae: 0.2457\n",
      "Epoch 170/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0948 - mae: 0.2350 - val_loss: 0.1016 - val_mae: 0.2428\n",
      "Epoch 171/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 0.0940 - mae: 0.2342 - val_loss: 0.1101 - val_mae: 0.2514\n",
      "Epoch 172/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0941 - mae: 0.2344 - val_loss: 0.1016 - val_mae: 0.2409\n",
      "Epoch 173/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 0.0940 - mae: 0.2339 - val_loss: 0.1032 - val_mae: 0.2426\n",
      "Epoch 174/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0940 - mae: 0.2339 - val_loss: 0.1064 - val_mae: 0.2466\n",
      "Epoch 175/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0936 - mae: 0.2339 - val_loss: 0.1087 - val_mae: 0.2504\n",
      "Epoch 176/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 0.0933 - mae: 0.2334 - val_loss: 0.1083 - val_mae: 0.2494\n",
      "Epoch 177/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0948 - mae: 0.2344 - val_loss: 0.1054 - val_mae: 0.2455\n",
      "Epoch 178/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 0.0939 - mae: 0.2338 - val_loss: 0.1061 - val_mae: 0.2467\n",
      "Epoch 179/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 10ms/step - loss: 0.0939 - mae: 0.2338 - val_loss: 0.1033 - val_mae: 0.2431\n",
      "Epoch 180/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0939 - mae: 0.2342 - val_loss: 0.1053 - val_mae: 0.2457\n",
      "Epoch 181/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 0.0940 - mae: 0.2341 - val_loss: 0.1033 - val_mae: 0.2435\n",
      "Epoch 182/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0940 - mae: 0.2334 - val_loss: 0.1044 - val_mae: 0.2449\n",
      "Epoch 183/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 11ms/step - loss: 0.0934 - mae: 0.2337 - val_loss: 0.1043 - val_mae: 0.2447\n",
      "Epoch 184/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 0.0934 - mae: 0.2329 - val_loss: 0.1026 - val_mae: 0.2425\n",
      "Epoch 185/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 85ms/step - loss: 0.0941 - mae: 0.2337 - val_loss: 0.1066 - val_mae: 0.2470\n",
      "Epoch 186/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0926 - mae: 0.2327 - val_loss: 0.1016 - val_mae: 0.2421\n",
      "Epoch 187/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0942 - mae: 0.2340 - val_loss: 0.1027 - val_mae: 0.2428\n",
      "Epoch 188/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.0936 - mae: 0.2338 - val_loss: 0.1023 - val_mae: 0.2423\n",
      "Epoch 189/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0937 - mae: 0.2339 - val_loss: 0.1119 - val_mae: 0.2546\n",
      "Epoch 190/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0935 - mae: 0.2333 - val_loss: 0.1093 - val_mae: 0.2505\n",
      "Epoch 191/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - loss: 0.0938 - mae: 0.2336 - val_loss: 0.1083 - val_mae: 0.2496\n",
      "Epoch 192/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0937 - mae: 0.2337 - val_loss: 0.1054 - val_mae: 0.2463\n",
      "Epoch 193/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0941 - mae: 0.2341 - val_loss: 0.1028 - val_mae: 0.2426\n",
      "Epoch 194/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0933 - mae: 0.2336 - val_loss: 0.1057 - val_mae: 0.2463\n",
      "Epoch 195/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0939 - mae: 0.2343 - val_loss: 0.1080 - val_mae: 0.2497\n",
      "Epoch 196/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0955 - mae: 0.2360 - val_loss: 0.1061 - val_mae: 0.2465\n",
      "Epoch 197/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0933 - mae: 0.2331 - val_loss: 0.0983 - val_mae: 0.2379\n",
      "Epoch 198/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - loss: 0.0932 - mae: 0.2333 - val_loss: 0.1002 - val_mae: 0.2398\n",
      "Epoch 199/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10ms/step - loss: 0.0936 - mae: 0.2337 - val_loss: 0.1019 - val_mae: 0.2422\n",
      "Epoch 200/200\n",
      "\u001b[1m3272/3272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 0.0933 - mae: 0.2331 - val_loss: 0.1122 - val_mae: 0.2541\n"
     ]
    }
   ],
   "source": [
    "# Define an improved ANN model with more neurons and layers\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),  # More neurons\n",
    "    keras.layers.Dropout(0.3),  # Prevent overfitting\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22f67c77-23ef-4f7c-a108-4b304eea4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3be41ac2-7294-4e56-be1e-ec9d6b93ba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1636/1636\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "ANN Model → RMSE: 9407.0456, R² Score: 0.5246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred_log = model.predict(X_test).flatten()\n",
    "\n",
    "# Convert log predictions back to original scale\n",
    "y_pred_original = np.expm1(y_pred_log)\n",
    "y_test_original = np.expm1(y_test)\n",
    "\n",
    "# Evaluate model\n",
    "rmse = mean_squared_error(y_test_original, y_pred_original, squared=False)\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "print(f\"ANN Model → RMSE: {rmse:.4f}, R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f436137-6eda-4d67-b69b-7ad1fdbf5793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
